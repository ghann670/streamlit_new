import streamlit as st
import pandas as pd
import plotly.express as px
import altair as alt
import matplotlib.pyplot as plt
from urllib.parse import parse_qs
import os, requests, io

# Page config
st.set_page_config(page_title="Usage Summary", page_icon="ğŸ“Š", layout="wide")

# ìºì‹œ í´ë¦¬ì–´ (ë””ë²„ê¹…ìš©)
if st.button("ğŸ”„ Clear Cache & Refresh"):
    st.cache_data.clear()
    st.rerun()

USAGE_LOCAL = "df_usage.xlsx"
USAGE_URL = "https://raw.githubusercontent.com/ghann670/streamlit/main/df_usage.xlsx"

USERS_LOCAL = "df_users.xlsx"
USERS_URL = "https://raw.githubusercontent.com/ghann670/streamlit_new/main/df_users.xlsx"

USERS_DATE_LOCAL = "users.xlsx"
USERS_DATE_URL = "https://raw.githubusercontent.com/ghann670/streamlit_new/main/users.xlsx"

@st.cache_data(show_spinner=False)
def load_trial_dates_df() -> pd.DataFrame:
    """Load trial dates data from users.xlsx date sheet"""
    # Try local first
    if os.path.exists(USERS_DATE_LOCAL):
        try:
            return pd.read_excel(USERS_DATE_LOCAL, sheet_name='date')
        except Exception as e:
            st.warning(f"Failed to read local '{USERS_DATE_LOCAL}' date sheet: {e}. Falling back to remoteâ€¦")
    # Fall back to remote
    try:
        r = requests.get(USERS_DATE_URL, timeout=30)
        r.raise_for_status()
        return pd.read_excel(io.BytesIO(r.content), sheet_name='date')
    except Exception as e:
        st.error("Failed to load trial dates data from both local file and remote URL.")
        st.stop()

@st.cache_data(show_spinner=False)
def load_users_df() -> pd.DataFrame:
    """Load users data from local file if available, else fall back to remote URL.
    Stops the app with an error message if both sources fail."""
    # Try local first
    if os.path.exists(USERS_LOCAL):
        try:
            return pd.read_excel(USERS_LOCAL)
        except Exception as e:
            st.warning(f"Failed to read local '{USERS_LOCAL}': {e}. Falling back to remoteâ€¦")
    # Fall back to remote
    try:
        r = requests.get(USERS_URL, timeout=30)
        r.raise_for_status()
        return pd.read_excel(io.BytesIO(r.content))
    except Exception as e:
        st.error("Failed to load users data from both local file and remote URL. Please check the data source.")
        st.stop()

@st.cache_data(show_spinner=False)
def load_usage_df() -> pd.DataFrame:
    """Load usage data from local file if available, else fall back to remote URL.
    Stops the app with an error message if both sources fail."""
    # Try local first
    if os.path.exists(USAGE_LOCAL):
        try:
            return pd.read_excel(USAGE_LOCAL)
        except Exception as e:
            st.warning(f"Failed to read local '{USAGE_LOCAL}': {e}. Falling back to remoteâ€¦")
    # Fall back to remote
    try:
        r = requests.get(USAGE_URL, timeout=30)
        r.raise_for_status()
        return pd.read_excel(io.BytesIO(r.content))
    except Exception as e:
        st.error("Failed to load usage data from both local file and remote URL. Please check the data source.")
        st.stop()


df_usage = load_usage_df()
df_users = load_users_df()
df_trial_dates = load_trial_dates_df()

# df_usage ì „ì²˜ë¦¬ (df_all ëŒ€ì‹  ì§ì ‘ ì‚¬ìš©)
# normalize helper
_norm = lambda s: "".join(str(s).strip().lower().replace("_", " ").split())
colmap = {_norm(c): c for c in df_usage.columns}
get = lambda name: colmap.get(_norm(name))

# rename to expected columns if present
rename_map = {}
for src, dst in [
    ("User Email", "user_email"),
    ("User Name", "user_name"),
    ("Organization", "organization"),
    ("Created At", "created_at"),
    ("Function Mode", "function_mode"),
    ("Selected Model", "selected_model"),
    ("Sender", "sender"),
    ("Time To First Byte", "time_to_first_byte"),
    ("Status", "status"),
    ("Division", "division"),
    ("Trial Start Date", "trial_start_date"),
]:
    src_col = get(src)
    if src_col:
        rename_map[src_col] = dst

df_usage = df_usage.rename(columns=rename_map)

# datetime parsing for df_usage
if "created_at" in df_usage.columns:
    df_usage['created_at'] = pd.to_datetime(df_usage['created_at'], errors='coerce', utc=True).dt.tz_localize(None)
if "trial_start_date" in df_usage.columns:
    df_usage['trial_start_date'] = pd.to_datetime(df_usage['trial_start_date'], errors='coerce')

# preprocessing for df_usage
df_usage['day_bucket'] = df_usage['created_at'].dt.date if 'created_at' in df_usage.columns else pd.NaT
df_usage['agent_type'] = df_usage['function_mode'].astype(str).str.split(":").str[0] if 'function_mode' in df_usage.columns else "normal"

# ì ˆê° ì‹œê°„ ë§¤í•‘
time_map = {"deep_research": 40, "pulse_check": 30}
df_usage["saved_minutes"] = df_usage["agent_type"].map(time_map).fillna(30)

# UI ì„¤ì •
st.title("\U0001F680 Usage Summary Dashboard")

# ì¡°ì§ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (df_usage ê¸°ì¤€)
org_event_counts = (
    df_usage
    .groupby('organization')
    .size()
    .sort_values(ascending=False)
)
org_list_sorted = org_event_counts.index.tolist()

# URLì—ì„œ selected_org íŒŒë¼ë¯¸í„° ì½ê¸°
default_org = st.query_params.get("selected_org", None)

# ê¸°ë³¸ ì„ íƒ ì¡°ì§ ì„¤ì •
if default_org in org_list_sorted:
    default_index = org_list_sorted.index(default_org)
else:
    default_index = 0

# ì¡°ì§ ì„ íƒ
selected_org = st.selectbox("Select Organization", org_list_sorted, index=default_index)

# ì„ íƒëœ ì¡°ì§ì˜ usage ë°ì´í„° í•„í„°ë§
df_usage_org = df_usage[df_usage['organization'] == selected_org]

# users.xlsxì˜ date ì‹œíŠ¸ì—ì„œ ì •í™•í•œ trial_start_date ê°€ì ¸ì˜¤ê¸°
org_trial_info = df_trial_dates[df_trial_dates['organization'] == selected_org]
if not org_trial_info.empty:
    trial_start_date = pd.to_datetime(org_trial_info['trial_start_date'].iloc[0])
else:
    # fallback: organizationì˜ ì²« ì´ë²¤íŠ¸ ë‚ ì§œ ì‚¬ìš©
    if not df_usage_org.empty and not df_usage_org['created_at'].isna().all():
        trial_start_date = df_usage_org['created_at'].min()
    else:
        trial_start_date = pd.Timestamp.now()
        
df_usage_org['trial_start_date'] = trial_start_date

# Metric ê³„ì‚° - df_users.xlsxì™€ df_usage ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •  
total_events = len(df_usage_org)  # All Events = ì„ íƒëœ ì¡°ì§ì˜ usage ë°ì´í„° ì „ì²´ ì¹´ìš´íŠ¸

# df_usersì—ì„œ ì„ íƒëœ organizationì˜ total users ê³„ì‚°
if 'organization' in df_users.columns:
    df_users_org = df_users[df_users['organization'] == selected_org]
else:
    # organization ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ ì‚¬ìš©ì
    df_users_org = df_users

total_users = df_users_org['user_email'].nunique()  # ì¤‘ë³µ ì œê±°

# df_usersì—ì„œ statusê°€ 'active'ì¸ ì‚¬ìš©ì ê³„ì‚° (ì¤‘ë³µ ì œê±°)
if 'status' in df_users_org.columns:
    active_users = df_users_org[df_users_org['status'] == 'active']['user_email'].nunique()
else:
    # status ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  ì‚¬ìš©ìë¥¼ activeë¡œ ê°„ì£¼
    active_users = total_users

active_ratio = f"{active_users} / {total_users}"

# df_usage_active ì •ì˜ (ì—¬ëŸ¬ ì„¹ì…˜ì—ì„œ ì‚¬ìš©)
if 'user_email' in df_users_org.columns and 'status' in df_users_org.columns:
    active_user_emails = df_users_org[df_users_org['status'] == 'active']['user_email'].tolist()
    df_usage_active = df_usage_org[df_usage_org['user_email'].isin(active_user_emails)]
else:
    df_usage_active = df_usage_org  # fallback to all usage

# Top user ê³„ì‚°
if not df_usage_active['user_name'].dropna().empty:
    top_user = df_usage_active['user_name'].value_counts().idxmax()
    top_user_count = df_usage_active['user_name'].value_counts().max()
    top_user_display = f"{top_user} ({top_user_count} times)"
else:
    top_user_display = "N/A"

# í‰ê·  ì´ë²¤íŠ¸
avg_events = round(total_events / active_users, 1) if active_users > 0 else 0

# ì ˆê° ì‹œê°„ (ì£¼ì°¨ ê³„ì‚° ëŒ€ì‹  ì „ì²´ ê¸°ê°„ ì‚¬ìš©)
if not df_usage_active.empty and active_users > 0:
    # ì „ì²´ ì‚¬ìš© ê¸°ê°„ì„ ì£¼ ë‹¨ìœ„ë¡œ ê³„ì‚°
    date_range = (df_usage_active['created_at'].max() - df_usage_active['created_at'].min()).days
    used_weeks = max(1, date_range // 7)  # ìµœì†Œ 1ì£¼
    total_saved_minutes = df_usage_active["saved_minutes"].sum()
    saved_minutes_per_user_per_week = round(total_saved_minutes / used_weeks / active_users, 1)
    saved_display = f"{saved_minutes_per_user_per_week} min"
else:
    saved_display = "â€”"

# âœ… Invited & No-Usage Users ì¶”ì¶œ - df_users.xlsx ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì • (ì¤‘ë³µ ì œê±°)
if 'status' in df_users_org.columns:
    invited_emails = df_users_org[df_users_org['status'] == 'invited_not_joined']['user_email'].dropna().unique() if 'user_email' in df_users_org.columns else []
    # joined but no usage = statusê°€ null/NaNì¸ ì‚¬ìš©ìë“¤
    joined_no_usage_emails = df_users_org[df_users_org['status'].isna()]['user_email'].dropna().unique() if 'user_email' in df_users_org.columns else []
else:
    # status ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´
    invited_emails = []
    joined_no_usage_emails = []

invited_display = ", ".join(invited_emails) if len(invited_emails) > 0 else "â€”"
joined_display = ", ".join(joined_no_usage_emails) if len(joined_no_usage_emails) > 0 else "â€”"

# Layout â€“ Metrics
col1, col2, col3 = st.columns(3)
col1.metric("All Events", total_events)
col2.metric("Active / Total Users", active_ratio)
col3.metric("Top User", top_user_display)

col4, col5, col6 = st.columns(3)
# earnings, briefings ì •ë³´ëŠ” df_usersì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ì œê±°)
earnings_users = df_users_org[df_users_org['earnings'] == 'onboarded']['user_email'].nunique() if 'earnings' in df_users_org.columns else 0
briefing_users = df_users_org[df_users_org['briefing'] == 'onboarded']['user_email'].nunique() if 'briefing' in df_users_org.columns else 0
col4.metric("Earnings/Briefing Users", f"{earnings_users}/{briefing_users}")
col5.metric("Avg. Events per Active User", avg_events)
col6.metric("Avg. Time Saved / User / Week", saved_display)

# User Status ì„¹ì…˜
st.markdown("### ğŸ‘¥ User Status")

# 2x2 ê·¸ë¦¬ë“œ ìƒì„±
status_col1, status_col2 = st.columns(2)

with status_col1:
    # ì™¼ìª½ ì—´
    st.markdown("**Invited but Not Joined**")
    st.markdown(
        f"""
        <div style='
            max-height: 60px;
            overflow-y: auto;
            font-size: 13px;
            border: 1px solid #ccc;
            padding: 6px;
            border-radius: 5px;
            background-color: #fffaf5;
            margin-bottom: 15px;
        '>
            {invited_display}
        </div>
        """,
        unsafe_allow_html=True
    )

    st.markdown("**Joined but No Usage**")
    st.markdown(
        f"""
        <div style='
            max-height: 60px;
            overflow-y: auto;
            font-size: 13px;
            border: 1px solid #ccc;
            padding: 6px;
            border-radius: 5px;
            background-color: #f5faff;
        '>
            {joined_display}
        </div>
        """,
        unsafe_allow_html=True
    )

with status_col2:
    # Normalë§Œ ì‚¬ìš©í•œ ìœ ì € ì°¾ê¸°
    all_users = df_usage_org['user_name'].unique()
    normal_only_users = []
    for user in all_users:
        user_types = df_usage_org[df_usage_org['user_name'] == user]['agent_type'].unique()
        if len(user_types) == 1 and user_types[0] == 'normal':
            normal_only_users.append(user)
    normal_only_users.sort()
    normal_only_display = ", ".join(normal_only_users) if normal_only_users else "â€”"

    # Recent 2 Weeks Active Users ì°¾ê¸° (ìµœê·¼ 2ì£¼ê°„ í™œì„± ì‚¬ìš©ì)
    if not df_usage_active.empty and 'created_at' in df_usage_active.columns:
        # ìµœê·¼ 2ì£¼ê°„ í™œì„± ì‚¬ìš©ì ì°¾ê¸°
        recent_date = df_usage_active['created_at'].max()
        if pd.notna(recent_date):
            two_weeks_ago = recent_date - pd.Timedelta(days=14)
            recent_users = df_usage_active[df_usage_active['created_at'] >= two_weeks_ago]['user_name'].dropna().unique()
            # ì•ˆì „í•œ ì •ë ¬ì„ ìœ„í•´ ë¬¸ìì—´ ë³€í™˜ í›„ ë¹ˆ ê°’ ì œê±°
            recent_users_clean = [str(user) for user in recent_users if pd.notna(user) and str(user).strip()]
            consistent_display = ", ".join(sorted(recent_users_clean)) if len(recent_users_clean) > 0 else "â€”"
        else:
            consistent_display = "â€”"
    else:
        consistent_display = "â€”"

    # ì˜¤ë¥¸ìª½ ì—´
    st.markdown("**Recent 2 Weeks Active Users**")
    st.markdown(
        f"""
        <div style='
            max-height: 60px;
            overflow-y: auto;
            font-size: 13px;
            border: 1px solid #ccc;
            padding: 6px;
            border-radius: 5px;
            background-color: #f5fff5;
            margin-bottom: 15px;
        '>
            {consistent_display}
        </div>
        """,
        unsafe_allow_html=True
    )

    st.markdown("**Normal Function Only Users**")
    st.markdown(
        f"""
        <div style='
            max-height: 60px;
            overflow-y: auto;
            font-size: 13px;
            border: 1px solid #ccc;
            padding: 6px;
            border-radius: 5px;
            background-color: #fff5ff;
        '>
            {normal_only_display}
        </div>
        """,
        unsafe_allow_html=True
    )



# Total usage ì‹œê³„ì—´ ì°¨íŠ¸
st.markdown("---")
st.subheader("ğŸ“… Total Usage Over Time (All Functions)")

# 1ï¸âƒ£ ë‚ ì§œë³„ ì „ì²´ ì‚¬ìš©ëŸ‰ ì§‘ê³„
end_date = pd.Timestamp.now()
default_start = pd.Timestamp('2025-01-01')

# ì¡°ì§ë³„ trial_start_date í™•ì¸
df_active_org = df_usage_active.copy()

# 2024ë…„ trial_start_dateë¥¼ ê°€ì§„ ì¡°ì§ì€ 2025-01-01ë¶€í„° ì‹œì‘í•˜ë„ë¡ ì¡°ì •
df_active_org.loc[df_active_org['trial_start_date'].dt.year == 2024, 'trial_start_date'] = default_start

# ê° ì¡°ì§ë³„ë¡œ ë°ì´í„° ì²˜ë¦¬
org_data_list = []
for org in df_active_org['organization'].unique():
    org_df = df_active_org[df_active_org['organization'] == org]
    
    try:
        # trial_start_date ì²˜ë¦¬
        org_start = org_df['trial_start_date'].iloc[0]
        
        # end_date ì²˜ë¦¬ (í˜„ì¬ ì‹œê°„ìœ¼ë¡œë¶€í„°)
        end_date = pd.Timestamp.now()
        
        # org_startê°€ ìœ íš¨í•œì§€ í™•ì¸í•˜ê³  ì²˜ë¦¬
        if pd.isna(org_start) or not isinstance(org_start, pd.Timestamp):
            # created_atì—ì„œ ìµœì†Œê°’ ì°¾ê¸°
            if not org_df['created_at'].empty:
                org_start = pd.to_datetime(org_df['created_at'].min())
            else:
                # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
                continue
        
        # ë‚ ì§œê°€ ìœ íš¨í•œì§€ ìµœì¢… í™•ì¸
        if pd.isna(org_start) or pd.isna(end_date):
            print(f"Invalid dates for org {org}: start={org_start}, end={end_date}")
            continue
            
        # ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ì€ ê²½ìš° ì²˜ë¦¬
        if org_start > end_date:
            org_start = end_date
        
        # í•´ë‹¹ ì¡°ì§ì˜ ë‚ ì§œ ë²”ìœ„ ìƒì„±
        org_dates = pd.date_range(
            start=org_start.normalize(),  # ì‹œê°„ ì •ë³´ ì œê±°
            end=end_date.normalize(),     # ì‹œê°„ ì •ë³´ ì œê±°
            freq='D'
        )
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì²˜ë¦¬
        org_date_df = pd.DataFrame({'created_at': org_dates})
        
        # í•´ë‹¹ ì¡°ì§ì˜ ì‹¤ì œ ë°ì´í„° ì§‘ê³„
        org_counts = org_df.groupby(org_df["created_at"].dt.date).size().reset_index(name="count")
        org_counts["created_at"] = pd.to_datetime(org_counts["created_at"])
        
        # ë°ì´í„° ë³‘í•©
        org_daily = pd.merge(org_date_df, org_counts, on='created_at', how='left')
        org_data_list.append(org_daily)
        
    except Exception as e:
        print(f"Error processing org {org}: {str(e)}")
        continue

# ëª¨ë“  ì¡°ì§ì˜ ë°ì´í„° í•©ì¹˜ê¸°
df_total_daily = pd.concat(org_data_list)
df_total_daily = df_total_daily.groupby('created_at')['count'].sum().reset_index()
df_total_daily['count'] = df_total_daily['count'].fillna(0)

# âœ… 2ï¸âƒ£ ë‚ ì§œ ë¼ë²¨ ìƒì„± (ì˜ˆ: 7/11)
df_total_daily["date_label"] = df_total_daily["created_at"].dt.strftime("%-m/%d")  # macOS/Linux
# ìœˆë„ìš°ëŠ” "%#m/%d"

# âœ… Plotly ì‹œê³„ì—´ ì°¨íŠ¸ (yì¶• ìƒë‹¨ ì—¬ìœ  í¬í•¨)
fig1 = px.line(
    df_total_daily,
    x="created_at",  # date_label ëŒ€ì‹  created_at ì‚¬ìš©
    y="count",
    markers=True,
    labels={"created_at": "Date", "count": "Total Event Count"},
)

# âœ… ì°¨íŠ¸ ë ˆì´ì•„ì›ƒ ì„¤ì •
fig1.update_layout(
    height=300,
    width=900,
    xaxis=dict(
        rangeslider=dict(visible=True),  # í•˜ë‹¨ì— ìŠ¬ë¼ì´ë” ì¶”ê°€
        type="date",
        tickformat="%Y-%m-%d",
                    range=[df_total_daily['created_at'].min(), df_total_daily['created_at'].max()]  # xì¶• ë²”ìœ„ ì„¤ì •
    ),
    margin=dict(l=50, r=50, t=30, b=50)  # ì—¬ë°± ì¡°ì •
)

# âœ… yì¶• ë²”ìœ„ ìë™ë³´ë‹¤ ì¡°ê¸ˆ ë” í¬ê²Œ ì„¤ì •
max_count = df_total_daily["count"].max()
fig1.update_yaxes(range=[0, max_count + 10])

st.plotly_chart(fig1, use_container_width=True)




# âœ… New Section: ìœ ì €ë³„ ë¼ì¸ì°¨íŠ¸ ì¶”ê°€
st.markdown("### ğŸ‘¥ Users' Daily Usage (2025 Data Only)")

# ìœ ì €ë³„ ì¼ë³„ ì‚¬ìš©ëŸ‰ ì§‘ê³„ (ê° ìœ ì €ì˜ ì²« ì‚¬ìš©ì¼ë¶€í„° í˜„ì¬ê¹Œì§€)
# ì‹¤ì œ ì‚¬ìš©ëŸ‰ ë°ì´í„° ì§‘ê³„ (2025ë…„ ë°ì´í„°ë§Œ)
df_2025 = df_usage_active[df_usage_active['created_at'].dt.year == 2025]

# ê° ìœ ì €ì˜ ì²« ì‚¬ìš©ì¼ ì°¾ê¸° (2025ë…„ ê¸°ì¤€)
user_first_dates = df_2025.groupby('user_name')['created_at'].min().reset_index()
user_first_dates['created_at'] = user_first_dates['created_at'].dt.date
user_counts = df_2025.groupby(
    [df_2025["created_at"].dt.date, "user_name"]
).size().reset_index(name="count")

# ìœ ì €ë³„ total usage ìˆ˜ ê¸°ì¤€ ì •ë ¬
user_total_counts = user_counts.groupby("user_name")["count"].sum()
sorted_users = user_total_counts.sort_values(ascending=False).index.tolist()
default_users = sorted_users[:3]  # ìƒìœ„ 3ëª… ê¸°ë³¸ ì„ íƒ

# ì„¸ì…˜ ìƒíƒœì— ì„ íƒ ìœ ì € ëª©ë¡ ì €ì¥
if "selected_users" not in st.session_state:
    st.session_state.selected_users = default_users

# ì „ì²´ ì„ íƒ / í•´ì œ ë²„íŠ¼
col1, col2 = st.columns([1, 1])
with col1:
    if st.button("âœ… ì „ì²´ ì„ íƒ"):
        st.session_state.selected_users = sorted_users
with col2:
    if st.button("âŒ ì „ì²´ í•´ì œ"):
        st.session_state.selected_users = []

# ë©€í‹°ì…€ë ‰íŠ¸ (ì„¸ì…˜ ìƒíƒœë¡œ ë™ê¸°í™”, ìœ íš¨ì„± ë³´ì •)
valid_default_users = [user for user in st.session_state.selected_users if user in sorted_users]

selected_users = st.multiselect(
    "Select users to display",
    options=sorted_users,
    default=valid_default_users,
    key="selected_users"
)

# 2025ë…„ ë°ì´í„°ì˜ ìµœëŒ€ ë‚ ì§œ ì‚¬ìš©
actual_end_date = df_2025['created_at'].max().date() if not df_2025.empty else pd.Timestamp.now().date()

# ì„ íƒëœ ìœ ì €ë“¤ì˜ ë°ì´í„°ë§Œ ì²˜ë¦¬
df_user_daily_list = []
for user in selected_users:
    user_start = user_first_dates[user_first_dates['user_name'] == user]['created_at'].iloc[0]
    user_dates = pd.date_range(start=user_start, end=actual_end_date, freq='D')
    
    # í•´ë‹¹ ìœ ì €ì˜ ë‚ ì§œë³„ ë°ì´í„° ìƒì„±
    user_df = pd.DataFrame({'created_at': user_dates})
    user_df['user'] = user
    
    # ì‹¤ì œ ë°ì´í„°ì™€ ë³‘í•©
    user_counts_filtered = user_counts[user_counts['user_name'] == user].copy()
    user_counts_filtered['created_at'] = pd.to_datetime(user_counts_filtered['created_at'])
    
    user_df = pd.merge(
        user_df,
        user_counts_filtered[['created_at', 'count']],
        on='created_at',
        how='left'
    )
    df_user_daily_list.append(user_df)

# ëª¨ë“  ìœ ì € ë°ì´í„° í•©ì¹˜ê¸°
if df_user_daily_list:
    df_user_filtered = pd.concat(df_user_daily_list, ignore_index=True)
    df_user_filtered['count'] = df_user_filtered['count'].fillna(0)
    df_user_filtered["date_label"] = df_user_filtered["created_at"].dt.strftime("%-m/%d")
else:
    df_user_filtered = pd.DataFrame(columns=['created_at', 'user', 'count', 'date_label'])

# âœ… ë¼ì¸ì°¨íŠ¸ ì‹œê°í™”
if df_user_filtered.empty:
    st.info("No data for selected users.")
else:
    chart_users = alt.Chart(df_user_filtered).mark_line(point=True).encode(
        x=alt.X(
            "created_at:T",
            title="Date",
            axis=alt.Axis(
                labelAngle=0,
                format="%m/%d",
                tickCount={"interval": "day", "step": 1},  # í•˜ë£¨ ê°„ê²©ìœ¼ë¡œ ëˆˆê¸ˆ í‘œì‹œ
                grid=True
            )
        ),
        y=alt.Y("count:Q", title="Event Count"),
        color=alt.Color("user:N", title="User", sort=sorted_users),
        tooltip=[
            alt.Tooltip("created_at:T", title="Date", format="%Y-%m-%d"),
            alt.Tooltip("user:N", title="User"),
            alt.Tooltip("count:Q", title="Count")
        ]
    ).properties(width=900, height=300)
    
    st.altair_chart(chart_users, use_container_width=True)

    # í…Œì´ë¸” ì¶”ê°€
    st.markdown("#### Daily Usage Table")
    
    # í”¼ë²— í…Œì´ë¸” ìƒì„±
    df_user_filtered['date_col'] = df_user_filtered['created_at'].dt.strftime("%m/%d")
    table_data = df_user_filtered.pivot_table(
        index='user',
        columns='date_col',
        values='count',
        fill_value=0
    )
    
    # ë‚ ì§œ ì»¬ëŸ¼ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
    date_columns = [col for col in table_data.columns if col != 'Total']
    # ì‹¤ì œ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë‚ ì§œì˜ ì—°ë„ë¥¼ ì°¾ì•„ì„œ ì •ë ¬
    def get_actual_date_for_sorting(date_str):
        matching_dates = df_user_filtered[df_user_filtered['created_at'].dt.strftime("%m/%d") == date_str]['created_at']
        if not matching_dates.empty:
            return matching_dates.min()
        else:
            return pd.to_datetime(f"2025/{date_str}")  # 2025ë…„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë³€ê²½
    
    sorted_date_columns = sorted(date_columns, key=get_actual_date_for_sorting)
    
    # Total ì»¬ëŸ¼ ì¶”ê°€
    table_data['Total'] = table_data.sum(axis=1)
    
    # Total ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    table_data = table_data.sort_values('Total', ascending=False)
    
    # Totalì„ ë§¨ ì•ìœ¼ë¡œ, ê·¸ ë‹¤ìŒ ë‚ ì§œë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ë°°ì¹˜
    cols = ['Total'] + sorted_date_columns
    table_data = table_data[cols]
    
    # Total í–‰ ì¶”ê°€
    table_data.loc['Total'] = table_data.sum(numeric_only=True)
    
    # 0ì„ '-'ë¡œ ëŒ€ì²´í•˜ê¸° ì „ì— ì •ìˆ˜ë¡œ ë³€í™˜
    table_data = table_data.apply(lambda x: x.astype(float).astype(int) if pd.api.types.is_numeric_dtype(x) else x)
    table_data = table_data.replace(0, '-')
    
    st.dataframe(table_data.astype(object), use_container_width=True)




# í•¨ìˆ˜ ë° ì£¼ê°„ ì‹œê³„ì—´
st.markdown("---")

# Trial Start Date ê³„ì‚°
try:
    trial_start = pd.to_datetime(df_usage_org['trial_start_date'].iloc[0]).strftime('%Y-%m-%d')
except (IndexError, pd.errors.OutOfBoundsDatetime):
    trial_start = pd.Timestamp.now().strftime('%Y-%m-%d')

# View Mode ì„ íƒ
view_mode = st.radio(
    "Select View Mode",
    ["Recent 4 Weeks", f"Trial Period (Trial Start Date: {trial_start})"],
    horizontal=True,
    key="function_trends_view_mode"
)

st.subheader("ğŸ“ˆ Weekly Function Usage Trends")

# ì£¼ì°¨ ë²”ìœ„ ì„¤ì • (Recent 4 Weeks ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)
if view_mode == "Recent 4 Weeks":
    # ê¸°ì¤€ ë‚ ì§œ: ì˜¤ëŠ˜ ë‚ ì§œ ì •ì˜¤ ê¸°ì¤€
    now = pd.Timestamp.now().normalize() + pd.Timedelta(hours=12)
    
    # ê° ì£¼ì°¨ ë²”ìœ„ ì„¤ì •
    week_ranges = {
        'week4': (now - pd.Timedelta(days=6), now),
        'week3': (now - pd.Timedelta(days=13), now - pd.Timedelta(days=7)),
        'week2': (now - pd.Timedelta(days=20), now - pd.Timedelta(days=14)),
        'week1': (now - pd.Timedelta(days=27), now - pd.Timedelta(days=21)),
    }
    
    # ì£¼ì°¨ ë²„í‚· í• ë‹¹ í•¨ìˆ˜
    def assign_week_bucket(date):
        if pd.isna(date):
            return None
        # dateëŠ” ì´ë¯¸ timezone-naive ìƒíƒœ
        for week, (start, end) in week_ranges.items():
            if start <= date <= end:
                return week
        return None
    
    # ì´ ì„¹ì…˜ì—ì„œë§Œ week_bucket í• ë‹¹
    df_usage_org['week_bucket'] = df_usage_org['created_at'].apply(assign_week_bucket)

if view_mode == f"Trial Period (Trial Start Date: {trial_start})":
    # trial_start_date ê¸°ì¤€ìœ¼ë¡œ ì£¼ì°¨ ê³„ì‚°
    df_usage_org['week_from_trial'] = ((df_usage_org['created_at'] - df_usage_org['trial_start_date'])
                                .dt.days // 7 + 1)
    
    # trial_start_dateì™€ ê°™ì€ ë‚ ì§œ(0)ë„ 1ì£¼ì°¨ë¡œ ì²˜ë¦¬
    df_usage_org.loc[df_usage_org['week_from_trial'] <= 1, 'week_from_trial'] = 1
    
    # week í¬ë§·íŒ… (ì†Œìˆ˜ì  ì œê±°)
    df_usage_org['week_from_trial'] = df_usage_org['week_from_trial'].fillna(1)  # nanì„ 1ë¡œ ì²˜ë¦¬
    df_usage_org['week_from_trial'] = df_usage_org['week_from_trial'].map(lambda x: f'Trial Week {int(x)}')
    
    df_chart = df_usage_org.groupby(['week_from_trial', 'agent_type']).size().reset_index(name='count')
    
    # ëˆ„ë½ëœ week_from_trial, agent_type ì¡°í•© ì±„ì›Œë„£ê¸°
    all_weeks = sorted(df_usage_org['week_from_trial'].unique())
    all_agents = df_chart['agent_type'].unique()
    all_combinations = pd.MultiIndex.from_product([all_weeks, all_agents], names=['week_from_trial', 'agent_type']).to_frame(index=False)
    df_chart = pd.merge(all_combinations, df_chart, on=['week_from_trial', 'agent_type'], how='left')
    df_chart['count'] = df_chart['count'].fillna(0).astype(int)
else:
    df_chart = df_usage_org.groupby(['week_bucket', 'agent_type']).size().reset_index(name='count')

    # ëˆ„ë½ëœ week_bucket, agent_type ì¡°í•© ì±„ì›Œë„£ê¸°
    all_weeks = list(week_ranges.keys())
    all_agents = df_chart['agent_type'].unique()
    all_combinations = pd.MultiIndex.from_product([all_weeks, all_agents], names=['week_bucket', 'agent_type']).to_frame(index=False)
    df_chart = pd.merge(all_combinations, df_chart, on=['week_bucket', 'agent_type'], how='left')
    df_chart['count'] = df_chart['count'].fillna(0).astype(int)

# Pivot Table - ëª¨ë“œì— ë”°ë¼ ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½
if view_mode == "Recent 4 Weeks":
    df_week_table = df_chart.pivot_table(
        index='agent_type',
        columns='week_bucket',
        values='count',
        fill_value=0,
        aggfunc='sum'
    )
else:
    df_week_table = df_chart.pivot_table(
        index='agent_type',
        columns='week_from_trial',
        values='count',
        fill_value=0,
        aggfunc='sum'
    )

df_week_table['Total'] = df_week_table.sum(axis=1)
df_week_table = df_week_table.sort_values('Total', ascending=False)

# Trial Week ì»¬ëŸ¼ë“¤ì„ ìˆ«ì ìˆœì„œë¡œ ì •ë ¬
if view_mode != "Recent 4 Weeks":
    # Trial Week ì»¬ëŸ¼ë“¤ë§Œ ì¶”ì¶œí•˜ê³  ìˆ«ìë¡œ ì •ë ¬
    trial_week_cols = [col for col in df_week_table.columns if col != 'Total' and 'Trial Week' in str(col)]
    # Trial Week ìˆ«ì ì¶”ì¶œí•´ì„œ ì •ë ¬
    trial_week_cols.sort(key=lambda x: int(x.split()[-1]))
    df_week_table = df_week_table[['Total'] + trial_week_cols]
else:
    df_week_table = df_week_table[['Total'] + [col for col in df_week_table.columns if col != 'Total']]

df_week_table.loc['Total'] = df_week_table.sum(numeric_only=True)

# 0ì„ '-'ë¡œ ëŒ€ì²´
df_week_table = df_week_table.replace(0, '-')

# ì°¨íŠ¸ ì •ë ¬ ìˆœì„œ ì„¤ì •
sorted_agent_order = df_week_table.drop("Total").index.tolist()

if view_mode == "Recent 4 Weeks":
    df_chart['agent_type'] = pd.Categorical(df_chart['agent_type'], categories=sorted_agent_order, ordered=True)
    df_chart = df_chart.sort_values('agent_type')
    
    left, right = st.columns([6, 6])
    with left:
        chart_week = alt.Chart(df_chart).mark_line(point=True).encode(
            x=alt.X('week_bucket:N', title='Week', axis=alt.Axis(labelAngle=0)),
            y=alt.Y('count:Q', title='Event Count'),
            color=alt.Color('agent_type:N', title='Function', sort=sorted_agent_order),
            tooltip=['agent_type', 'count']
        ).properties(width=600, height=300)
        
        st.altair_chart(chart_week, use_container_width=True)
else:
    df_chart['agent_type'] = pd.Categorical(df_chart['agent_type'], categories=sorted_agent_order, ordered=True)
    df_chart = df_chart.sort_values(['week_from_trial', 'agent_type'])
    
    left, right = st.columns([6, 6])
    with left:
        chart_week = alt.Chart(df_chart).mark_line(point=True).encode(
            x=alt.X('week_from_trial:N', title='Trial Week', axis=alt.Axis(labelAngle=0)),
            y=alt.Y('count:Q', title='Event Count'),
            color=alt.Color('agent_type:N', title='Function', sort=sorted_agent_order),
            tooltip=['agent_type', 'count']
        ).properties(width=600, height=300)
        
        st.altair_chart(chart_week, use_container_width=True)

with right:
    st.dataframe(df_week_table.astype(object), use_container_width=True)


# ğŸ“Š Daily usage ì‹œê³„ì—´
st.subheader("ğŸ“Š Daily Function Usage for a Selected Week")



# ğŸ“… ì£¼ì°¨ ì„ íƒ - view modeì— ë”°ë¼ ë‹¤ë¥´ê²Œ
if view_mode == "Recent 4 Weeks":
    # week_bucketì´ ì—†ìœ¼ë©´ ìƒì„±
    if 'week_bucket' not in df_usage_org.columns:
        # ê¸°ì¤€ ë‚ ì§œ: ì˜¤ëŠ˜ ë‚ ì§œ ì •ì˜¤ ê¸°ì¤€
        now = pd.Timestamp.now().normalize() + pd.Timedelta(hours=12)
        
        # ê° ì£¼ì°¨ ë²”ìœ„ ì„¤ì •
        week_ranges = {
            'week4': (now - pd.Timedelta(days=6), now),
            'week3': (now - pd.Timedelta(days=13), now - pd.Timedelta(days=7)),
            'week2': (now - pd.Timedelta(days=20), now - pd.Timedelta(days=14)),
            'week1': (now - pd.Timedelta(days=27), now - pd.Timedelta(days=21)),
        }
        
        # ì£¼ì°¨ ë²„í‚· í• ë‹¹ í•¨ìˆ˜
        def assign_week_bucket(date):
            if pd.isna(date):
                return None
            for week, (start, end) in week_ranges.items():
                if start <= date <= end:
                    return week
            return None
        
        df_usage_org['week_bucket'] = df_usage_org['created_at'].apply(assign_week_bucket)
    
    week_options = sorted(df_usage_org['week_bucket'].dropna().unique(), reverse=True)
    selected_week = st.selectbox("Select Week", week_options, key="daily_select_week")
    
    # ì„ íƒëœ ì£¼ì°¨ì˜ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    week_start, week_end = week_ranges[selected_week]
    week_dates = pd.date_range(week_start, week_end).date
else:
    # Trial Period Mode
    # ëª¨ë“  ê°€ëŠ¥í•œ Trial Week ìƒì„± (1ì£¼ì°¨ë¶€í„° í˜„ì¬ê¹Œì§€)
    max_week = ((pd.Timestamp.now() - df_usage_org['trial_start_date'].min()).days // 7) + 1
    week_options = [f'Trial Week {i}' for i in range(max_week, 0, -1)]
    selected_week = st.selectbox("Select Week", week_options, key="daily_select_week")
    
    # ì„ íƒëœ Trial Weekì˜ ìˆ«ì ì¶”ì¶œ
    week_num = int(selected_week.split()[-1])
    
    # í•´ë‹¹ ì£¼ì°¨ì˜ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    trial_start = pd.to_datetime(df_usage_org['trial_start_date'].iloc[0])
    week_start = trial_start + pd.Timedelta(days=(week_num-1)*7)
    week_end = week_start + pd.Timedelta(days=6)
    week_dates = pd.date_range(week_start, week_end).date

# ğŸ“† ì„ íƒëœ ì£¼ê°„ ë°ì´í„° í•„í„°ë§ (df_usage_active ì‚¬ìš©)
df_week = df_usage_active[df_usage_active['created_at'].dt.date.isin(week_dates)]

# ğŸ“Š ì¼ë³„-ê¸°ëŠ¥ë³„ ì§‘ê³„
agent_types = df_usage_active['agent_type'].unique()  # ì „ì²´ ê¸°ëŠ¥ ëª©ë¡ ì‚¬ìš©

# ì„ íƒëœ ì£¼ì˜ ëª¨ë“  ë‚ ì§œì™€ ê¸°ëŠ¥ ì¡°í•© ìƒì„±
date_range = pd.date_range(start=min(week_dates), end=max(week_dates), freq='D')
all_combinations = pd.MultiIndex.from_product(
    [date_range, agent_types],
    names=['created_at', 'agent_type']
).to_frame(index=False)

# ì‹¤ì œ ë°ì´í„° ì§‘ê³„
df_day = df_week.groupby([df_week['created_at'].dt.date, 'agent_type']).size().reset_index(name='count')
df_day['created_at'] = pd.to_datetime(df_day['created_at'])

# ëª¨ë“  ë‚ ì§œ-ê¸°ëŠ¥ ì¡°í•©ì— ëŒ€í•´ ë°ì´í„° ë³‘í•© (ì—†ëŠ” ë‚ ì§œëŠ” 0ìœ¼ë¡œ í‘œì‹œ)
df_day = pd.merge(all_combinations, df_day, on=['created_at', 'agent_type'], how='left')
df_day['count'] = df_day['count'].fillna(0).astype(int)

# ğŸ“Š ê¸°ëŠ¥ë³„ ì •ë ¬ ê¸°ì¤€ ê³„ì‚° (ë§ì´ ì“´ ìˆœì„œ â†’ ì•„ë˜ì¸µë¶€í„° ìŒ“ì„)
agent_order_by_volume = (
    df_day.groupby('agent_type')['count']
    .sum()
    .sort_values(ascending=False)
    .index.tolist()
)
agent_order_for_stack = agent_order_by_volume  # ë§ì´ ì“´ ìˆœì„œëŒ€ë¡œ ìŠ¤íƒ

# ğŸ” ì •ë ¬ ìˆœì„œ ì ìš©
df_day['agent_type'] = pd.Categorical(
    df_day['agent_type'],
    categories=agent_order_for_stack,
    ordered=True
)
df_day = df_day.sort_values(['created_at', 'agent_type'], ascending=[True, True])

# ğŸ“ˆ Plotly ì°¨íŠ¸ + ğŸ“‹ í…Œì´ë¸”
left2, right2 = st.columns([6, 6])
with left2:
    # ğŸ“Š Plotly stacked bar chart
    fig_day = px.bar(
        df_day,
        x="created_at",
        y="count",
        color="agent_type",
        category_orders={"agent_type": agent_order_for_stack},
        color_discrete_sequence=px.colors.qualitative.Set1,
        labels={"created_at": "Date", "count": "Event Count", "agent_type": "Function"},
    )
    
    # ì°¨íŠ¸ ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig_day.update_layout(
        barmode="stack",
        width=600,
        height=300,
        xaxis_title="Date",
        yaxis_title="Event Count",
        legend_title="Function",
        showlegend=True if df_day['count'].sum() > 0 else False  # ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ë²”ë¡€ í‘œì‹œ
    )
    
    # xì¶• ë‚ ì§œ í¬ë§· ì„¤ì •
    fig_day.update_xaxes(
        tickformat="%m-%d",
        type='date',
        dtick="D1",  # í•˜ë£¨ ê°„ê²©ìœ¼ë¡œ ëˆˆê¸ˆ í‘œì‹œ
        range=[min(week_dates), max(week_dates)]  # ì„ íƒëœ ì£¼ì˜ ì „ì²´ ë‚ ì§œ ë²”ìœ„ í‘œì‹œ
    )
    
    st.plotly_chart(fig_day, use_container_width=True)

with right2:
    # ğŸ“Š ì§‘ê³„ í…Œì´ë¸” ì¤€ë¹„
    df_day_table = df_day.pivot_table(
        index='agent_type',
        columns='created_at',
        values='count',
        fill_value=0,
        aggfunc='sum'
    )
    
    # ì»¬ëŸ¼ ì´ë¦„ì„ mm-dd í˜•ì‹ìœ¼ë¡œ ë³€ê²½
    df_day_table.columns = pd.to_datetime(df_day_table.columns).strftime('%m-%d')
    
    # ë‚ ì§œ ì»¬ëŸ¼ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ì‹¤ì œ ë°ì´í„°ì˜ ì—°ë„ ì‚¬ìš©)
    date_columns = [col for col in df_day_table.columns if col != 'Total']
    # ì‹¤ì œ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë‚ ì§œì˜ ì—°ë„ë¥¼ ì°¾ì•„ì„œ ì •ë ¬
    def get_actual_date_for_day_sorting(date_str):
        # mm-dd í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì— ëŒ€í•´ ì‹¤ì œ ë°ì´í„°ì—ì„œ í•´ë‹¹í•˜ëŠ” ì „ì²´ ë‚ ì§œ ì°¾ê¸°
        matching_dates = df_day[df_day['created_at'].dt.strftime("%m-%d") == date_str]['created_at']
        if not matching_dates.empty:
            return matching_dates.min()  # ê°€ì¥ ì´ë¥¸ ë‚ ì§œ ì‚¬ìš©
        else:
            return pd.to_datetime(f"2024-{date_str}")  # ê¸°ë³¸ê°’
    
    sorted_date_columns = sorted(date_columns, key=get_actual_date_for_day_sorting)
    
    df_day_table['Total'] = df_day_table.sum(axis=1)
    df_day_table = df_day_table.sort_values('Total', ascending=False)
    df_day_table = df_day_table[['Total'] + sorted_date_columns]
    df_day_table.loc['Total'] = df_day_table.sum(numeric_only=True)

    # 0ì„ '-'ë¡œ ëŒ€ì²´
    df_day_table = df_day_table.replace(0, '-')

    st.dataframe(df_day_table.astype(object), use_container_width=True)


# ğŸ‘¥ Function Usage by User
st.subheader("ğŸ‘¥ Function Usage by User")

# ì „ì²´ ìœ ì € ë¦¬ìŠ¤íŠ¸ (ëª¨ë“  ì£¼ì°¨ì˜ ìœ ì €ë¥¼ í¬í•¨í•˜ë„ë¡)
all_users = sorted(df_usage_org['user_name'].dropna().unique())

# ì„¸ì…˜ ìƒíƒœì— ì„ íƒëœ ìœ ì € ì €ì¥
if "selected_user_for_function" not in st.session_state:
    st.session_state.selected_user_for_function = "All Users"

# ìœ ì € í•„í„° ì¶”ê°€ (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©)
selected_user = st.selectbox(
    "Select User (Optional)", 
    ["All Users"] + all_users,
    key="selected_user_for_function"
)

# ğŸ“… ì£¼ì°¨ ì„ íƒ - view modeì— ë”°ë¼ ë‹¤ë¥´ê²Œ
if view_mode == "Recent 4 Weeks":
    week_options = sorted(df_usage_org['week_bucket'].dropna().unique(), reverse=True)
    selected_week = st.selectbox("Select Week", week_options, key="user_week_select")
    
    # ì„ íƒëœ ì£¼ì°¨ì˜ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    week_start, week_end = week_ranges[selected_week]
    week_dates = pd.date_range(week_start, week_end).date
else:
    # Trial Period Mode
    # Trial Week ìˆ«ì ì¶”ì¶œí•´ì„œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    week_options = sorted(df_usage_org['week_from_trial'].unique(), 
                         key=lambda x: int(x.split()[-1]),
                         reverse=True)
    selected_week = st.selectbox("Select Week", week_options, key="user_week_select")
    
    # ì„ íƒëœ Trial Weekì˜ ìˆ«ì ì¶”ì¶œ
    week_num = int(selected_week.split()[-1])
    
    # í•´ë‹¹ ì£¼ì°¨ì˜ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
    trial_start = pd.to_datetime(df_usage_org['trial_start_date'].iloc[0])
    week_start = trial_start + pd.Timedelta(days=(week_num-1)*7)
    week_end = week_start + pd.Timedelta(days=6)
    week_dates = pd.date_range(week_start, week_end).date

# ì„ íƒëœ ì£¼ê°„ ë°ì´í„° í•„í„°ë§
df_user_week = df_usage_org[df_usage_org['created_at'].dt.date.isin(week_dates)]

# ê¸°ë³¸ ì§‘ê³„ ë°ì´í„° ì¤€ë¹„ (ì „ì²´ ìœ ì €)
df_user_stack_full = df_user_week.groupby(['user_name', 'agent_type']).size().reset_index(name='count')

# ğŸ‘‰ ê¸°ëŠ¥ ì •ë ¬ ê¸°ì¤€ ì •ì˜ (ë§ì´ ì“´ ìˆœ)
sorted_func_order = (
    df_user_stack_full.groupby('agent_type')['count']
    .sum().sort_values(ascending=False).index.tolist()
)

# âœ… ì™¼ìª½: ì°¨íŠ¸ìš© - top 10 ìœ ì €ë§Œ í•„í„°ë§
top_users = (
    df_user_stack_full.groupby('user_name')['count']
    .sum().nlargest(10).index.tolist()
)
df_user_stack_chart = df_user_stack_full[df_user_stack_full['user_name'].isin(top_users)].copy()
df_user_stack_chart['agent_type'] = pd.Categorical(
    df_user_stack_chart['agent_type'],
    categories=sorted_func_order,
    ordered=True
)
df_user_stack_chart = df_user_stack_chart.sort_values(['user_name', 'agent_type'])

# ğŸ“Š ì‹œê°í™”
left, right = st.columns([7, 5])
with left:
    fig = px.bar(
        df_user_stack_chart,
        x="user_name",
        y="count",
        color="agent_type",
        category_orders={
            "agent_type": sorted_func_order,
            "user_name": top_users
        },
        color_discrete_sequence=px.colors.qualitative.Set1,
        labels={"user_name": "User", "count": "Usage Count", "agent_type": "Function"},
    )
    fig.update_layout(
        barmode="stack",
        xaxis_title="User",
        yaxis_title="Usage Count",
        legend_title="Function",
        height=350,
    )
    st.plotly_chart(fig, use_container_width=True)

with right:
    if selected_user == "All Users":
        # ì „ì²´ ìœ ì € ìš”ì•½ í…Œì´ë¸”
        df_user_table = df_user_stack_full.pivot_table(
            index='agent_type',  # agent_typeì„ í–‰ìœ¼ë¡œ
            columns='user_name',  # userë¥¼ ì—´ë¡œ
            values='count',
            aggfunc='sum',
            fill_value=0
        )
        
        # Total ì»¬ëŸ¼ ì¶”ê°€ ë° ì •ë ¬
        df_user_table['Total'] = df_user_table.sum(axis=1)
        df_user_table = df_user_table.sort_values('Total', ascending=False)
        df_user_table = df_user_table[['Total'] + [col for col in df_user_table.columns if col != 'Total']]
        
        # Total í–‰ ì¶”ê°€
        df_user_table.loc['Total'] = df_user_table.sum(numeric_only=True)
        
    else:
        # ì„ íƒëœ ìœ ì €ì˜ ì¼ë³„ ìƒì„¸ ë°ì´í„°
        df_user_detail = df_user_week[df_user_week['user_name'] == selected_user]
        
        # ëª¨ë“  ë‚ ì§œì™€ agent_type ì¡°í•© ìƒì„±
        all_dates = pd.date_range(week_start, week_end).strftime('%m/%d')
        all_agent_types = sorted_func_order
        all_combinations = pd.MultiIndex.from_product(
            [all_agent_types, all_dates],
            names=['agent_type', 'date']
        ).to_frame(index=False)
        
        # ì‹¤ì œ ë°ì´í„° ì§‘ê³„
        df_user_detail['date'] = df_user_detail['created_at'].dt.strftime('%m/%d')
        df_user_counts = df_user_detail.groupby(['agent_type', 'date']).size().reset_index(name='count')
        
        # ëª¨ë“  ì¡°í•©ê³¼ ì‹¤ì œ ë°ì´í„° ë³‘í•©
        df_user_counts = pd.merge(
            all_combinations,
            df_user_counts,
            on=['agent_type', 'date'],
            how='left'
        ).fillna(0)
        
        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        df_user_table = df_user_counts.pivot_table(
            index='agent_type',
            columns='date',
            values='count',
            fill_value=0
        )
        
        # ë‚ ì§œ ì»¬ëŸ¼ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ (ì‹¤ì œ ë°ì´í„°ì˜ ì—°ë„ ì‚¬ìš©)
        date_columns = [col for col in df_user_table.columns if col != 'Total']
        # ì‹¤ì œ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë‚ ì§œì˜ ì—°ë„ë¥¼ ì°¾ì•„ì„œ ì •ë ¬
        def get_actual_date_for_user_sorting(date_str):
            # mm/dd í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì— ëŒ€í•´ ì‹¤ì œ ë°ì´í„°ì—ì„œ í•´ë‹¹í•˜ëŠ” ì „ì²´ ë‚ ì§œ ì°¾ê¸°
            matching_dates = df_user_detail[df_user_detail['created_at'].dt.strftime("%m/%d") == date_str]['created_at']
            if not matching_dates.empty:
                return matching_dates.min()  # ê°€ì¥ ì´ë¥¸ ë‚ ì§œ ì‚¬ìš©
            else:
                return pd.to_datetime(f"2024/{date_str}")  # ê¸°ë³¸ê°’
        
        sorted_date_columns = sorted(date_columns, key=get_actual_date_for_user_sorting)
        
        # Total ì»¬ëŸ¼ ì¶”ê°€ ë° ì •ë ¬
        df_user_table['Total'] = df_user_table.sum(axis=1)
        df_user_table = df_user_table.sort_values('Total', ascending=False)
        df_user_table = df_user_table[['Total'] + sorted_date_columns]
        
        # Total í–‰ ì¶”ê°€
        df_user_table.loc['Total'] = df_user_table.sum(numeric_only=True)
    
    st.dataframe(df_user_table.astype(object), use_container_width=True)


# ğŸ“Š Response Time Analysis
st.markdown("---")
st.subheader("ğŸ“ˆ LinqAlpha Response Time Analysis")

# ë°ì´í„° ì „ì²˜ë¦¬
df_time = df_all.copy()
# ì´ìƒì¹˜ ì²˜ë¦¬
df_time.loc[df_time['time_to_first_byte'] <= 0, 'time_to_first_byte'] = None
df_time.loc[df_time['time_to_first_byte'] > 300000, 'time_to_first_byte'] = None  # 5ë¶„ ì´ˆê³¼ ì œê±°

# msë¥¼ ì´ˆë¡œ ë³€í™˜
df_time['time_to_first_byte'] = df_time['time_to_first_byte'] / 1000

# ê¸°ë³¸ í†µê³„ëŸ‰ í‘œì‹œ
col1, col2, col3 = st.columns(3)
with col1:
    median_time = df_time['time_to_first_byte'].median()
    st.metric("Median Response Time", f"{median_time:.1f} sec")
with col2:
    mean_time = df_time['time_to_first_byte'].mean()
    st.metric("Average Response Time", f"{mean_time:.1f} sec")
with col3:
    p95_time = df_time['time_to_first_byte'].quantile(0.95)
    st.metric("95th Percentile", f"{p95_time:.1f} sec")

# Matplotlibì„ ì‚¬ìš©í•œ ì‹œê³„ì—´ ì‹œê°í™”
plt.style.use('seaborn')
fig_mpl, ax = plt.subplots(figsize=(12, 6))

# ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
df_time['date'] = df_time['created_at'].dt.date
daily_stats = df_time.groupby('date').agg({
    'time_to_first_byte': 'median',
    'id': 'count'
}).reset_index()

# 2025ë…„ 4ì›” 1ì¼ ì´í›„, ì˜¤ëŠ˜ ì œì™¸ ë°ì´í„°ë§Œ í•„í„°ë§
start_date = pd.Timestamp('2025-04-01').date()
end_date = pd.Timestamp.now().date() - pd.Timedelta(days=1)
daily_stats = daily_stats[
    (daily_stats['date'] >= start_date) & 
    (daily_stats['date'] <= end_date)
]

# ì‹œê³„ì—´ í”Œë¡¯
ax.plot(daily_stats['date'], daily_stats['time_to_first_byte'], 
        marker='o', linestyle='-', linewidth=2, markersize=6)

# ì¶• ë ˆì´ë¸”ê³¼ ì œëª©
ax.set_xlabel('Date')
ax.set_ylabel('Response Time (seconds)')
ax.set_title('Daily Median Response Time (Matplotlib)')

# xì¶• ë‚ ì§œ í¬ë§· ì¡°ì •
plt.xticks(rotation=45)
plt.tight_layout()

# Streamlitì— Matplotlib ì°¨íŠ¸ í‘œì‹œ
st.pyplot(fig_mpl)


# ë‚ ì§œ ì„ íƒê¸° ì¶”ê°€
available_dates = sorted(daily_stats['date'].unique(), reverse=True)  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
selected_date = st.selectbox(
    "Select a date to see detailed statistics",
    [""] + [d.strftime("%Y-%m-%d") for d in available_dates],
    index=0
)

# ì„ íƒëœ ë‚ ì§œê°€ ìˆì„ ê²½ìš°ì—ë§Œ ìƒì„¸ í†µê³„ í‘œì‹œ
if selected_date:
    selected_date = pd.to_datetime(selected_date).date()
    selected_date_data = df_time[df_time['date'] == selected_date].copy()
    
    st.markdown(f"### ğŸ“Š Detailed Analysis for {selected_date}")
    
    # Functionë³„ í†µê³„ í…Œì´ë¸” ë¨¼ì € ê³„ì‚°
    func_stats = selected_date_data.groupby('agent_type').agg({
        'time_to_first_byte': ['count', 'mean', 'median', 'max']
    }).round(1)
    func_stats.columns = ['Count', 'Mean (sec)', 'Median (sec)', 'Max (sec)']
    
    # ì›ë³¸ ë°ì´í„° ìˆ˜ì™€ í•„í„°ë§ëœ ë°ì´í„° ìˆ˜ ê³„ì‚°
    total_raw_requests = len(selected_date_data)
    valid_requests = func_stats['Count'].sum()
    
    # ê¸°ë³¸ í†µê³„
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric(
            "Total Requests",
            f"{total_raw_requests} ({valid_requests} valid)",
            help="Total number of requests (number of requests with valid response time)"
        )
    with col2:
        st.metric("Median Response Time", f"{selected_date_data['time_to_first_byte'].median():.1f} sec")
    with col3:
        st.metric("Mean Response Time", f"{selected_date_data['time_to_first_byte'].mean():.1f} sec")
    with col4:
        st.metric("Max Response Time", f"{selected_date_data['time_to_first_byte'].max():.1f} sec")

    # Functionë³„ í†µê³„ í…Œì´ë¸” ì—´ ìˆœì„œ ë³€ê²½
    func_stats = func_stats[['Count', 'Median (sec)', 'Mean (sec)', 'Max (sec)']]
    func_stats = func_stats.sort_values('Count', ascending=False)
    
    # 0ì„ '-'ë¡œ ëŒ€ì²´
    func_stats = func_stats.replace(0, '-')
    
    st.markdown("#### Function Statistics")
    st.dataframe(func_stats.astype(object), use_container_width=True)

    # Slow Requests (ìƒìœ„ 10ê°œ)
    st.markdown("#### Slowest Requests")
    slow_requests = selected_date_data.nlargest(10, 'time_to_first_byte')[
        ['created_at', 'agent_type', 'time_to_first_byte', 'id']
    ].copy()
    slow_requests['created_at'] = slow_requests['created_at'].dt.strftime('%Y-%m-%d %H:%M:%S')
    slow_requests.columns = ['Timestamp', 'Function', 'Response Time (sec)', 'Request ID']
    slow_requests = slow_requests.sort_values('Response Time (sec)', ascending=False)
    st.dataframe(slow_requests, use_container_width=True)

# ë‘ ë²ˆì§¸ ì¤„: íˆìŠ¤í† ê·¸ë¨ê³¼ ë„í‘œ
left_col, right_col = st.columns([3, 2])  # íˆìŠ¤í† ê·¸ë¨ì´ ë” ë„“ê²Œ

with left_col:
    # íˆìŠ¤í† ê·¸ë¨
    fig2 = px.histogram(
        df_time,
        x='time_to_first_byte',
        nbins=50,
        title='Distribution of Response Times',
        labels={'time_to_first_byte': 'Response Time (seconds)', 'count': 'Number of Requests'}
    )
    
    # ì¤‘ì•™ê°’ê³¼ í‰ê· ê°’ í‘œì‹œì„  ì¶”ê°€
    fig2.add_vline(
        x=median_time,
        line=dict(color="red", width=2, dash="dash"),
        annotation_text=f"Median: {median_time:.1f}s",
        annotation_position="top",
        annotation=dict(font=dict(color="red"))
    )
    fig2.add_vline(
        x=mean_time,
        line=dict(color="green", width=2, dash="dash"),
        annotation_text=f"Mean: {mean_time:.1f}s",
        annotation_position="bottom",
        annotation=dict(font=dict(color="green"))
    )
    
    fig2.update_layout(
        height=400,
        bargap=0.1,
        showlegend=True
    )
    st.plotly_chart(fig2, use_container_width=True)

with right_col:
    # í•¨ìˆ˜ë³„ ì‘ë‹µ ì‹œê°„ (Count ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
    st.markdown("### ğŸ” Response Time by Function")
    func_stats = df_time.groupby('agent_type')['time_to_first_byte'].agg([
        'mean', 'median', 'count'
    ]).reset_index()
    func_stats.columns = ['Function', 'Mean (sec)', 'Median (sec)', 'Count']
    func_stats = func_stats.sort_values('Count', ascending=False)  # Count ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ
    
    # 0ì„ '-'ë¡œ ëŒ€ì²´
    func_stats = func_stats.replace(0, '-')
    
    st.dataframe(func_stats.round(2).astype(object), use_container_width=True, hide_index=True)  # hide_index=True ì¶”ê°€

